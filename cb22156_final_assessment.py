# -*- coding: utf-8 -*-
"""CB22156_Final_Assessment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E2HlPjLHzgQuPRIopLgtoQ64hEnLMXYC
"""

# !pip install streamlit

"""Import libraries"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
import streamlit as st

from sklearn import metrics
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix

from sklearn.preprocessing import StandardScaler

"""Descriptive Analysis"""

file = 'personality_dataset.csv'

df = pd.read_csv(file)

print(df)

# Descibe the dataframe
df.describe()

"""Dataframe info"""

df.info()

"""Handling Categorical Data"""

df['Stage_fear'] = df['Stage_fear'].replace({'Yes' : 1, 'No': 0})
print(df['Stage_fear'])

df['Drained_after_socializing'] = df['Drained_after_socializing'].replace({'Yes' : 1, 'No': 0})
print(df['Drained_after_socializing'])

df['Personality'] = df['Personality'].replace({'Extrovert': 1, 'Introvert': 0})
print(df['Personality'])

"""Handling Imputer"""

from sklearn.impute import SimpleImputer


# print(df)


imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')

imp_mean.fit(df)

new_df = imp_mean.transform(df)

filtered_df = pd.DataFrame(new_df, columns=df.columns, index=df.index)

print(filtered_df)

"""Handling Missing Values"""

#Show current dataframe rows
print(f'Current rows: {len(filtered_df)}')

#Filtering / Handling missing values
cleaned_df = filtered_df.dropna()

print(f'Cleaned rows: {len(cleaned_df)}')
print(cleaned_df)

"""Handling NULL values"""

print(f"Current row count: {len(cleaned_df)}")

# Filter with NULL value

nan_value = float("NaN")

cleaned_df.replace("", nan_value, inplace=True)

cleaned_df.dropna(how='all', axis=1, inplace=True)

# display(cleaned_df)

print(f"After cleaning again: {len(cleaned_df)}")

"""Two Algorithms Model

K-Nearest Neighbours
"""

# Set the proper x and y from dataframe
x = cleaned_df.iloc[:,:-1]
y = cleaned_df.iloc[:,7]

print(x)
print(y)

#Split the data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state = 42)

print(x_train)

# Feature Scaling

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

# âœ… Re-wrap as DataFrame with original column names and index
x_train = pd.DataFrame(x_train_scaled, columns=x_train.columns, index=x_train.index)
x_test = pd.DataFrame(x_test_scaled, columns=x_test.columns, index=x_test.index)

#Fitting KNN classifier to train

knn_classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)

knn_classifier.fit(x_train, y_train)

#Predict with x_test
knn_y_pred = knn_classifier.predict(x_test)

#Print the y prediction
# print(len(knn_y_pred))
# print(knn_y_pred)

"""Random Forest"""

# Create Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=50, criterion='entropy', random_state=0)

# Fit with the same train data
rf_classifier.fit(x_train, y_train)

# Predict the random data
rf_y_pred =  rf_classifier.predict(x_test)

# Print random forest y prediction
# print(len(rf_y_pred))
# print(rf_y_pred)

"""Confusion Matrix for both algorithm"""

#KNN Confusion Matrix
knn_cm = confusion_matrix(y_test, knn_y_pred)

print(knn_cm)

tp = knn_cm[0][0]
fp = knn_cm[0][1]
tn = knn_cm[1][0]
fn = knn_cm[1][1]

precision = ((tp) / (tp + fp))
recall = ((tp) / (tp + fn))

# Accuracy and Precision of K-Nearest Neighbours
print(f"Accuracy (%): {((tp + fn) / (tp + fp + tn + fn)) * 100}%")
print(f"Precision (%): {precision * 100}%")
print(f"Recall (%): {recall * 100}%")
print(f"F1 Score (%): {((2 * precision * recall) / (precision + recall)) * 100}%")

#RF Confusion Matrix
rf_cm = confusion_matrix(y_test, rf_y_pred)

print(rf_cm)

tp = rf_cm[0][0]
fp = rf_cm[0][1]
tn = rf_cm[1][0]
fn = rf_cm[1][1]

precision = ((tp) / (tp + fp))
recall = ((tp) / (tp + fn))

# Accuracy and Precision of Random Forest
print(f"Accuracy (%): {((tp + fn) / (tp + fp + tn + fn)) * 100}%")
print(f"Precision (%): {precision * 100}%")
print(f"Recall (%): {recall * 100}%")
print(f"F1 Score (%): {((2 * precision * recall) / (precision + recall)) * 100}%")

"""Display Confusion Matrix"""

knn_cm_display = metrics.ConfusionMatrixDisplay(
    confusion_matrix=knn_cm,
    display_labels=[0, 1])

knn_cm_display.plot()

rf_cm_display = metrics.ConfusionMatrixDisplay(
    confusion_matrix=rf_cm,
    display_labels=[0, 1])

rf_cm_display.plot()


plt.show()

"""Feature Selection for each algorithm"""

from sklearn.feature_selection import SequentialFeatureSelector

# Use KNN
knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)

# Sequential Forward Selection
sfs = SequentialFeatureSelector(knn, n_features_to_select=4, direction='forward')
sfs.fit(x_train, y_train)

# Get selected features
selected_features = x.columns[sfs.get_support()]
print("Selected features:", selected_features)

# Reduce dataset to selected features
knn_x_train_selected = x_train[selected_features]
knn_x_test_selected = x_test[selected_features]

print(knn_x_train_selected)
print(knn_x_test_selected)

# Random Forest built in

importances = rf_classifier.feature_importances_
features = x_train.columns

feature_importance_df = pd.DataFrame({'Feature': x_train.columns, 'Importance': importances})


# Create a DataFrame
importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Show top features
print(importance_df)

# Optional: select top 4 features
rf_x_train_selected = x_train[importance_df['Feature'].head(4)]
rf_x_test_selected = x_test[importance_df['Feature'].head(4)]

print(rf_x_train_selected)
print(rf_x_test_selected)

"""Retrained the algorithm using selected features only and calculate the accuracy

KNN algorithm
"""

# Create K-Nearest Neighbours Classifier
knn_after = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)

knn_after.fit(knn_x_train_selected, y_train)

#Predict with test data
knn_pred = knn_after.predict(knn_x_test_selected)

"""Random Forest Algorithm"""

# Create Random Forest Classifier
rf_after = RandomForestClassifier(n_estimators=50, criterion='entropy', random_state=0)

# Fit with the same train data
rf_after.fit(rf_x_train_selected, y_train)

# Predict the random data
rf_pred =  rf_after.predict(rf_x_test_selected)

# Print random forest y prediction
# print(len(rf_y_pred))
# print(rf_y_pred)

"""Confusion Matrix"""

#KNN Confusion Matrix
knn_cm_after = confusion_matrix(y_test, knn_pred)

print(knn_cm_after)

knn_tp = knn_cm_after[0][0]
knn_fp = knn_cm_after[0][1]
knn_tn = knn_cm_after[1][0]
knn_fn = knn_cm_after[1][1]

knn_precision = ((knn_tp) / (knn_tp + knn_fp))
knn_recall = ((knn_tp) / (knn_tp + knn_fn))

# Accuracy and Precision of K-Nearest Neighbours
print(f"Accuracy (%): {((knn_tp + knn_fn) / (knn_tp + knn_fp + knn_tn + knn_fn)) * 100}%")
print(f"Precision (%): {knn_precision * 100}%")
print(f"Recall (%): {knn_recall * 100}%")
print(f"F1 Score (%): {((2 * (knn_precision * knn_recall) / (knn_precision + knn_recall))) * 100}%")

#RF Confusion Matrix
rf_cm_after = confusion_matrix(y_test, rf_pred)

print(rf_cm_after)

tp = rf_cm_after[0][0]
fp = rf_cm_after[0][1]
tn = rf_cm_after[1][0]
fn = rf_cm_after[1][1]

precision = ((tp) / (tp + fp))
recall = ((tp) / (tp + fn))

# Accuracy and Precision of Random Forest
print(f"Accuracy (%): {((tp + fn) / (tp + fp + tn + fn)) * 100}%")
print(f"Precision (%): {precision * 100}%")
print(f"Recall (%): {recall * 100}%")
print(f"F1 Score (%): {((2 * (precision * recall) / (precision + recall))) * 100}%")

"""Final analysis

Based on the evaluation of both models using accuracy, precision, recall, and F1 score, the K-Nearest Neighbors (KNN) algorithm demonstrates superior performance, particularly in terms of accuracy (91.61%) and precision (89.10%). Although its recall is slightly lower than that of Random Forest, KNN provides a better balance through a higher F1 Score.

Therefore, KNN is selected for deployment due to its strong overall classification performance and better precision, which is crucial in applications where false positives need to be minimized. Additionally, its simplicity and ease of interpretation make it a practical choice for real-world deployment scenarios with moderate data size.

Streamlit App Building
"""

st.write("""
# Extrover vs Introvert Prediction App

The following app predicts either you are extrovert or introvert!
""")

st.sidebar.header('User Input Parameters')

def user_input_features():
    drained_after_socializing = st.sidebar.slider('Drained_after_socializing', 0, 1)
    social_event_attendance = st.sidebar.slider('Social_event_attendance', 0, 10)
    stage_fear = st.sidebar.slider('Stage_fear', 0, 1)
    time_spent_alone = st.sidebar.slider('Time_spent_Alone', 0, 11)
    # going_outside = st.sidebar.slider('Going_outside', 0.1, 2.5, 0.2)
    # friends_circle_size = st.sidebar.slider(' Friends_circle_size', 0.1, 2.5, 0.2)
    # post_frequency = st.sidebar.slider('Post_frequency', 0.1, 2.5, 0.2)
    data = {'Drained_after_socializing': drained_after_socializing,
            'Social_event_attendance': social_event_attendance,
            'Stage_fear': stage_fear,
            # 'Going_outside': going_outside,
            'Time_spent_Alone': time_spent_alone,}
            # 'Friends_circle_size': friends_circle_size}
            # 'Post_frequency': post_frequency}
    features = pd.DataFrame(data, index=[0])
    return features

df = user_input_features()

st.subheader('User Input Parameters')
st.write(df)

prediction = rf_after.predict(df)

prediction_proba = rf_after.predict_proba(df)

st.subheader('Class labels and their corresponding index number')
st.write(rf_x_train_selected.columns)

st.subheader('Prediction (1: Extrovert, 0: Introvert)')
st.write(prediction)


st.subheader('Prediction Probability (%)')
st.write(prediction_proba)